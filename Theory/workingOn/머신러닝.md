# 머신러닝

## 1. 단순 선형 회귀(Simple Linear Regression Model)

  단순 선형 회귀 : 입력 특성에 대한 선형함수를 만들어 예측을 하는 알고리즘
                             독립변수가 하나인 경우 특정 직선을 학습하는 것이다. 
                             y=b0+b1*x1

   다중 선형 회귀 : 하나의 종속 변수에 여러 개의 독립 변수를 갖는 경우
                            y= b0+ b1x1+b2x2+...+bnxn

> 분석에 회귀, 즉 함수식을 사용하는 이유
> 함수는 어떤 값 x에 대해 유일한 값 y를 결과로 갖는다. 
> 따라서 x에 따라 바뀌는 y를 예측하는데는 함수만한 게 없다.

선형 회귀 모델은 입력 특성의 가중치 합과 편향(또는 절편)을 더해 예측을 수행
회귀에서 널리 사용되는 평가지표는 MSE(평균 제곱 오차)로, MSE를 최소화하는 파라미터를 찾아야 한다. 
MSE를 최소화하기 위해 통계적 회귀분석에서는 정규 방정식을 사용하고, 머신러닝은 경사하강법을 사용한다. 
정규 방정식은 행렬을 이용하는 것이기 때문에 변수가 많을 수록 matrix 계산이 복잡해져 시간이 오래 걸리는 반면, 경사 하강법의 경우 학습횟수를 정해놓고 시작하기 때문에 정해진 시간 내에는 변수 개수와 상관없이 시간 내에 답이 나오는 장점이 있다.  

### 1) 정규 방정식

$$
\hat{\beta } = (X^TX)^-1X^Ty
$$

위의 방정식을 사용하면 MSE값을 최소로 하는 파라미터를 바로 얻을 수 있다.
sklearn의 LinearRegression을 이용하면 최소 자승법을 사용한 OLS(Ordinary Least Squares) 방식으로 선형 회귀 모델을 구현한다. 계수 w=(w1,..,wp)를 사용해 선형 모델을 피팅하여 데이터세트에서 관찰된 대상과 선형 근사에 의해 예측된 대상 간의 잔차제곱합을 최소화한다. 

#### sklearn의 LinearRegression

> sklearn.linear_model.LinearRegression(*, fit_intercept = True, normalize = 'deprecated' , copy_X = True, n_jobs = None, positive = False)

1. 매개변수 

   fit_intercept (bool,default = True) : False로 설정하면 절편을 사용하지 않는 모델로 계산하여 데이터가 원점을 지나 중앙에 위치한다. 즉 y = b*x 모델로 계산 
   normalize(bool,default = False) : True인 경우 X는 평균을 빼고 L2-norm으로 나누어 회귀 전에 정규화된 
                               False인 경우 정규화를 하지 않음.
                               표준화를 진행하고 싶으면, normalize=False로 설정한 뒤 fit()을 호출하기 전에 StandardSCaler()를 사용
   copy_X(bool,default = True) : True이면 X가 복사됨 False면 덮어씀
   n_jobs(int, defalut = None) : 계산 작업 횟수
   positive(bool, default=False) : True로 하면 계수가 양수가 된다.

2. 속성

   coef_ (array) : 추정된 계수, 대상이 2개 이상이면 2차원 배열, 1개면 길이가 n_features인 1차원 배열

   rank_ (int)     : 행렬 X의 rank
   singular_ (array) : 행렬 X의 특이값
   intercept_ (array) : 선형 모델의 독립항(절편)

3. 메서드
   fit(X,y[,sample_weight]) : 모델을 학습시킴 
      X : 2차원 배열의 학습 데이터 
      y : 타깃데이터
      sample_weight : 개별 데이터에 대한 가중치
     반환값 : object

   get_params([deep]) : 선형 회귀모델의 매개변수를 가져옴
     반환값 : 값에 매칭되는 파라미터 이름을 딕셔너리 형태로 변환
   predict(X) : 선형 모델을 사용해 예측
     X : 데이터 샘플
     반환값 : 예측값을 array로 반환

   score(X,y[,sample_weight]) : 예측의 결정계수를 반환
     X : 테스트 샘플을 array로 반환
     y : X의 실제값 
     sample_weight : 개별 가중치
     반환값 : 결정계수를 float로 반환 

```python
'''
https://www.kaggle.com/datasets/mirichoi0218/insurance?resource=download
insuarance 데이터로 나이에 따른 의료 비용 예측하기(Linear Regression) 
'''
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

data = pd.read_csv('../DATA/archive/insurance.csv')
data

x=data['age']
y=data['charges']

plt.figure(figsize =(10,5))
plt.scatter(x,y)
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

x = np.array(data['age'])
x.shape
#(1338,)
y = np.array(data['charges'])

#입력 데이터는 2차원 array이여야 하므로 reshape으로 차원 맞춰주기
x = x.reshape(1338,1)
y = y.reshape(1338,1)
lr = LinearRegression()
lr.fit(x,y)

#LinearRegression(copy_X = True, fit_intercept=True, n_jobs = None, normalize = False)

print('선형 회귀 모델 결과')
print('절편', lr.intercept_, "계수", lr.coef_)
print('결정계수',lr.score(x,y))

#19살과 64살의 의료 비용 예측하기
x_new = [[19],[64]]
y_hat = lr.predict(x_new)
print(y_hat)

#산점도 위에 회귀선을 그어 시각화 해보기
plt.figure(figsize =(10,5))
plt.plot(x_new,y_hat,"-r")
plt.plot(x,y,"b.")
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

![image-20221116125801039](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221116125801039.png)



### 2) 경사하강법

함수의 값이 낮아지는 방향으로 독립변수의 값을 바꿔가면서 최종적으로 최소 함숫값을 갖도록 하는 독립 변수값을 찾는 방식
여러 종류의 문제에서 최적의 해법을 찾을 수 있는 최적화 알고리즘으로, 함수를 최소화하기 위해 파라미터를 반복적으로 조정해나감.
분석에서 마주하는 함수들은 형태가 복잡해 수식으로 미분계수와 그 해를 찾기 어려울 수 있음. 데이터 양이 큰 경우 경사하강법이 상대적으로 쉽게 구현 가능
무작위초기화 방식을 통해 임의의 값으로 시작해 한번에 조금씩 함수의 값이 감소하는 방향으로 최솟값에 수렴할 때까지 점진적으로 진행
학습 스텝의 크기를 학습률(Learning Rate)라고 함. 학습률이 너무 작으면 수렴하기까지 반복을 여러 번 수행해야 하므로 시간이 오래 걸린다는 단점이 있음.
반대로 학습률을 너무 높이면 함수의 값이 발산되는 경향이 있음. 적당한 크기로 조절이 필요.

#### 경사하강법의 종류

##### a) 배치 경사하강법

  반복 시 전체 훈련세트를 사용해 가중값을 갱신함.
  계산량이 많아 훈련에 소요되는 시간이 증가함.
  학습 시 발생하는 잡음이 적은 최적치를 찾을 수 있음.

##### b)확률적 경사하강법

  한 개의 샘플데이터를 무작위로 선택하고 샘플의 경사를 선택
  반복때마다 가중값이 달라지기 때문에 비용함수가 최솟값에 접근할 때 확률값이 요동치며 평균적으로 감소함. 
  최솟값에 접근할 때 요동치기 때문에 알고리즘이   멈출 때가 최적치가 아닐 수 있음. 
  하지만 지역 최솟값을 건너 뛰고 전역 최솟값으로 다다를 가능성이 높고 데이터의 양이 많아도 계산 속도가 빠름

##### c) 미니 배치 경사하강법

  각 스텝을 반복할 때 임의의 30~50개 관측값으로 경사를 계산하고 모델의 가중값을 갱신
  파라미터 공간에서 확률적 경사하강법보다 지역 최솟값에서 빠져나오기 어려울 가능성이 있음.
  하지만 확률적 경사하강법보다 낮은 오차율을 가지게 되어 최솟값에 더 가까이 도달할 수 있음.

##### d) scikit-learn의 SGDRegressor

  SGDRegressor는 확률적 경사하강법을 사용한 방식으로 회귀모델을 구현. 
  손실의 기울기는 각 샘플에서 한 번에 추정되며 모델은 감소하는 강도, 즉, 학습률에 따라 업데이트.
  정규화는 제곱 유클리드 norm L2 또는 norm L1 또는 이 둘의 조합을 이용해 매개변수를 0벡터로 축소하는 손실 함수에 추가된 패널티

> *class* sklearn.linear_model.**SGDRegressor**(*loss='squared_error'*, ***, *penalty='l2'*, *alpha=0.0001*, *l1_ratio=0.15*, *fit_intercept=True*, *max_iter=1000*, *tol=0.001*, *shuffle=True*, *verbose=0*, *epsilon=0.1*, *random_state=None*, *learning_rate='invscaling'*, *eta0=0.01*, *power_t=0.25*, *early_stopping=False*, *validation_fraction=0.1*, *n_iter_no_change=5*, *warm_start=False*, *average=False*)

```python
import numpy as np
from sklearn.linear_model import SGDRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
# Always scale the input. The most convenient way is to use a pipeline.
reg = make_pipeline(StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3))
reg.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('sgdregressor', SGDRegressor())])
```

#sklearn의 pipline
 https://zephyrus1111.tistory.com/254
