# ADP 실기 17회

## 1.기계학습(집값 예측) - 30점

**시험 문제와 kaggle California Housing 데이터가 비슷해서 California 데이터로 해당 문제 풀어보기**

https://www.kaggle.com/code/ilialar/california-housing-analysis-and-preciction 참고

주택 가격 예측을 위한 ML 모델 생성/ 평가

- 데이터 : 방개수, 부엌, 모델링 여부, 집값
- 시각화, 전처리, 회귀 모델 평가, 규제, 앙상블, +a 등 3개의 모델을 training 시켜 결과 확인
- 종속변수가 양적변수이므로 회귀분석 모델 사용

### 1.1 EDA

EDA 및 데이터 전처리 (시각화 및 통계량 제시)

```python
#library load
import pandas as pd
import numpy as np

#data load & EDA
housingDf = pd.read_csv('archive/housing.csv')
housingDf.shape                  #행렬 갯수
housingDF.info()
housingDf.head()                 #일부 데이터
housingDf.median_house_value.describe()  #양적변수 통계정보(count,mean,std,사분위수값)
housingDf.ocean_proximity.value_counts() #질적변수 class count
housingDf['median_house_value'].hist() #막대그래프

import matplotlib.pyplot as plt
import seaborn as sns
sns.pairplot(housingDf, diag_kind='auto', hue='median_house_value') #산점도행렬
plt.show()

from pandas_profiling import ProfileReport
profile = ProfileReport(housingDf) #pandas profiling html로 저장
profile.to_file("californiaHousingReport.html")


#Preprocessing
''' 막대그래프로 확인했을 때, 분포가 정규화가 되지 않고, 500,000에서 잘린 것처럼 보임 
- 극단값 %확인하여 clipping 된 게 맞는 지 확인 
- normality test하여 정규성 확보
'''
housingDf['median_house_value'].hist() #막대그래프
max_target=housingDf['median_house_value'].max()
print("The largest median value:",max_target)
print("The # of values, equal to the largest:", sum(housingDf['median_house_value']==max_target))
print("The % of values, equal to the largest:", sum(housingDf['median_house_value']==max_target)/housingDf.shape[0])
'''
전체에서 제일 큰 수의 비율이 5%이므로 잘린 값이 맞음.
The largest median value: 500001.0
The # of values, equal to the largest: 742
The % of values, equal to the largest: 0.0479328165374677
'''
min_target=housingDf['median_house_value'].min()
print("The smallest median value:",min_target)
print("The # of values, equal to the smallest:", sum(housingDf['median_house_value']==min_target))
print("The % of values, equal to the smallest:", sum(housingDf['median_house_value']==min_target)/housingDf.shape[0])
'''
min값은 짤리지 않음.
The smallest median value: 14999.0
The # of values, equal to the smallest: 4
The % of values, equal to the smallest: 0.00025839793281653745
'''
##정규성 검정(normality test by QQ-plot, 다구스티노-피어슨 검정 (D'Agostino and Pearson's nomality test))
from statsmodels.graphics.gofplots import qqplot
from matplotlib import pyplot
qqplot(housingDf['median_house_value'], line='s')
pyplot.show()

from scipy.stats import normaltest

stat, p = normaltest(housingDf['median_house_value'])
print('Statistics=%.3f, p=%.3f' % (stat, p))

alpha = 0.05
if p < alpha:  # null hypothesis: x comes from a normal distribution
    print("The null hypothesis can be rejected")
else:
    print("The null hypothesis cannot be rejected")    
'''
Statistics=1831.228, p=0.000
The null hypothesis can be rejected
'''
#정규화
target_log=np.log1p(train_df['median_house_value'])
qqplot(target_log, line='s')
pyplot.show()

housingDf['median_house_value_log']=np.log1p(housingDf['median_house_value'])
'''
정규화를 해도 정규분포가 아니라 귀무가설을 기각가능한상태이다.(p=0)
정규화한 데이터를 쓰는 게 더 좋긴하지만, 모델 유효성 검증에서 한 번 더 확인해야함.
'''
#독립변수 정규화
numerical_features=list(housingDf.columns)
numerical_features.remove('ocean_proximity')
numerical_features.remove('median_house_value')
print(numerical_features)
#['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']
#막대그래프 확인 시, 정규분포가 아닌 것들 정규화하기 
housingDf[numerical_features].hist(bins=50, figsize=(10, 10))
skewed_features=['households','median_income','population', 'total_bedrooms', 'total_rooms']
log_numerical_features=[]
for f in skewed_features:
    train_df[f + '_log']=np.log1p(train_df[f])
    test_df[f + '_log']=np.log1p(test_df[f])
    log_numerical_features.append(f + '_log')
#housing_median_age도 clipped되어 있는 걸로 보여서 이진화함. 
max_house_age=housingDf['housing_median_age'].max()
print("The largest value:",max_house_age)
print("The # of values, equal to the largest:", sum(housingDf['housing_median_age']==max_house_age))
print("The % of values, equal to the largest:", sum(housingDf['housing_median_age']==max_house_age)/housingDf.shape[0])
'''
The largest value: 52.0
The # of values, equal to the largest: 978
The % of values, equal to the largest: 0.0631782945736434
'''
housingDf['age_clipped']=housingDf['housing_median_age']==max_house_age
#상관행렬 확인
import matplotlib.pyplot as plt
import seaborn as sns

corr_y = pd.DataFrame(housingDf).corr()
plt.rcParams['figure.figsize'] = (20, 16)  # Размер картинок
sns.heatmap(corr_y, 
            xticklabels=corr_y.columns.values,
            yticklabels=corr_y.columns.values, annot=True)
'''
- 집값은 중위소득과 상당한 상관관계가 있습니다.
- 가구 수는 인구와 100% 상관 관계가 없으므로 average_size_of_household를 기능으로 추가할 수 있습니다.
- 경도와 위도는 별도로 분석해야 함(대상 변수와의 상관관계만 파악하는 것은 그다지 유용하지 않음)
- 방의 수, 침실, 인구 및 가구와 같이 상관관계가 높은 일련의 기능이 있습니다. 특히 선형 모델을 사용하는 경우 이 하위 집합의 차원을 줄이는 것이 유용할 수 있습니다.
- total_bedrooms는 이러한 상관관계가 높은 기능 중 하나이며, 가장 간단한 선형 회귀를 사용하여 NaN 값을 높은 정밀도로 채울 수 있음을 의미합니다.
'''

#NaN값 선형회귀로 보간 작업 
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lin = LinearRegression()

# we will train our model based on all numerical non-target features with not NaN total_bedrooms
appropriate_columns = train_df.drop(['median_house_value','median_house_value_log',
                            'ocean_proximity', 'total_bedrooms_log'],axis=1)
train_data=appropriate_columns[~pd.isnull(train_df).any(axis=1)]

# model will be validated on 25% of train dataset 
# theoretically we can use even our test_df dataset (as we don't use target) for this task, but we will not
temp_train, temp_valid = train_test_split(train_data,shuffle = True, test_size = 0.25, random_state=17)

lin.fit(temp_train.drop(['total_bedrooms'],axis=1), temp_train['total_bedrooms'])
np.sqrt(mean_squared_error(lin.predict(temp_valid.drop(['total_bedrooms'],axis=1)),temp_valid['total_bedrooms']))
'''
선형회귀값으로 채운 후의 RMSE 값은 64.5
64.53336471582229
'''
#NaN값 평균값으로 보간
np.sqrt(mean_squared_error(np.ones(len(temp_valid['total_bedrooms']))*temp_train['total_bedrooms'].mean(), temp_valid['total_bedrooms']))
'''
평균값으로 채운 후의 RMSE값은 425, 따라서 선형회귀값이 더 낫다. 
425.3238618340849
'''

#modeling
#train-test set 분할
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(full_df,shuffle = True, test_size = 0.25, random_state=17)
train_df=train_df.copy()
test_df=test_df.copy()
print(train_df.shape)
print(test_df.shape)

#모델 적합
lin.fit(train_data.drop(['total_bedrooms'],axis=1), train_data['total_bedrooms'])
train_df['total_bedrooms_is_nan']=pd.isnull(train_df).any(axis=1).astype(int)
test_df['total_bedrooms_is_nan']=pd.isnull(test_df).any(axis=1).astype(int)
train_df['total_bedrooms'].loc[pd.isnull(train_df).any(axis=1)]=\
lin.predict(train_df.drop(['median_house_value','median_house_value_log','total_bedrooms','total_bedrooms_log','ocean_proximity','total_bedrooms_is_nan'],axis=1)[pd.isnull(train_df).any(axis=1)])
test_df['total_bedrooms'].loc[pd.isnull(test_df).any(axis=1)]=\
lin.predict(test_df.drop(['median_house_value','median_house_value_log','total_bedrooms','total_bedrooms_log','ocean_proximity','total_bedrooms_is_nan'],axis=1)[pd.isnull(test_df).any(axis=1)])

#linear regression can lead to negative predictions, let's change it
test_df['total_bedrooms']=test_df['total_bedrooms'].apply(lambda x: max(x,0))
train_df['total_bedrooms']=train_df['total_bedrooms'].apply(lambda x: max(x,0))

#evaluation 
```



### 1.2  모델 생성

####   1.2.1  데이터 분할

####  1.2.2  교호 작용을 고려한 다중 선형 회귀 수정

 2차 교호작용항 까지 고려한 회귀분석 수행 및 변수 선택 과정 제시

####  1.2.3   3가지 분류 모델 생성 및 비교, 좋은 모델 선택

벌점, 앙상블을 포함하여 모형에 적합한 기계학습 모델 3가지 (MSE, MAPE, R2 제시)





------


## 2. 시각화 및 시계열 분석(코로나 데이터) - 20점

시각화, 시계열, 비시계열 예측 모델 문제

- 데이터 : 국가별, 일별, 인구수, 확진자수, 사망자수, 완치자수, 검사자수

- 시각화, 전처리

  

### 2.1. 전체 인구대비 누적 사망률이 가장 높은 5개 국가 추출 후, 국가별 일일확진자, 누적확진자, 일일사망자, 누적사망자 시계열 그래프 출력

인구대비 코로나 확진자 비율이 가장 높은 국가 5개 제시하고 일일확진자, 누적확진자, 일일 사망자, 누적 사망자 추이를 각각 1장씩의 시각화 그래프로 시각화(차분을 이용함)

### 2.2 위험지수 생성 및 해석

코로나 위험지수를 개발하고 위험지수가 높은 국가 10개를 추려내서 막대그래프로 시각화하기

### 2.3 시계열 /비선형 모델 생성

한국 확진자수 예측하는 문제





------


## 3. 통계분석(설문데이터 분석) - 50점

분석 전, 역코딩을 반영해야함.

### 3.1 그룹별 영역별 기술 통계량( 평균, 표준편차, 왜도, 첨도 산출)

### 3.2 요인분석

응답항목별 차이가 있는지 분석

(아마 Anova Table을 요구하는 것 같습니다)

### 3.3 신뢰성 지수 산출

각 영역별 correlation의 평균과 개수

참고 : https://didalsgur.tistory.com/entry/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A0%84%EB%AC%B8%EA%B0%80ADP-17%ED%9A%8C-%EC%8B%A4%EA%B8%B0%EC%8B%9C%ED%97%98-%ED%9B%84%EA%B8%B0

https://0dood0.tistory.com/150

https://ysyblog.tistory.com/227



 다구스티노-피어슨(D'Agostino Pearson) 검정

- 정규성 검정 방법

- 왜도와 첨도이 정규분포와 일치하는 지를 판단하는 검정방법

- 데이터 갯수가 20개 이상일 때 유효

- **
  귀무가설(H0) : 데이터가 정규분포를 따른다.**

  **대립가설(H1) : 데이터가 정규분포를 따르지 않는다.**

- 왜도(g1), 첨도(g2) 계산 

![image-20230215155235887](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230215155235887.png)

![image-20230215155446464](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230215155446464.png)

![image-20230215155536276](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230215155536276.png)

R로 코딩 

R에서 다구스티노-피어슨 검정을 실시하는 함수는 fBasics 패키지의 dagoTest() 함수입니다. 아래와 같이 데이터를 dagoTest() 함수를 입력하여 실행하면 검정 통계량 및 p-value를 결과가 표시하게 됩니다. 그럼 정규분포 난수 데이터(norm_num)와 지수분포 데이터(exp_num)을 만들어 각각  다구스티노-피어슨 검정을 실시하여 보겠습니다.

```R
> library(fBasics)
> norm_num <- rnorm(1000, mean=1, sd=2)
> dagoTest(norm_num)

Title:
 D'Agostino Normality Test

Test Results:
  STATISTIC:
    Chi2 | Omnibus: 1.9095
    Z3  | Skewness: -0.2169
    Z4  | Kurtosis: -1.3647
  P VALUE:
    Omnibus  Test: 0.3849 
    Skewness Test: 0.8283 
    Kurtosis Test: 0.1723 

Description:
 Sun May 31 14:28:48 2020 by user: pmw72

> exp_num <- rexp(1000,rate=2)
> dagoTest(exp_num)

Title:
 D'Agostino Normality Test

Test Results:
  STATISTIC:
    Chi2 | Omnibus: 422.2247
    Z3  | Skewness: 17.2252
    Z4  | Kurtosis: 11.2035
  P VALUE:
    Omnibus  Test: < 2.2e-16 
    Skewness Test: < 2.2e-16 
    Kurtosis Test: < 2.2e-16 

Description:
 Sun May 31 14:28:49 2020 by user: pmw72
```

위에서 각각 다구스티노-피어슨 통계량(Chi2), 왜도(Skewness), 첨도(Kurtosis) 통계량이 나타나며 이에 대한 유의확률(p-value)가 나타나게 됩니다. 다구스티노-피어슨 통계량만으로 판단하게 되면 정규분포 난수(norm_num)은 유의확률이 0.3849 > 0.05임으로 정규분포를 따른다고 판단할 수 있으며 지수분포 난수는 (exp_num)은 <0.05임으로 정규분포를 따른다고 할 수 없습니다. 

참고 : https://m.blog.naver.com/pmw9440/221984027319
